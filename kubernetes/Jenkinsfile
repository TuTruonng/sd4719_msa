pipeline {
  agent any

  // static environment values (change region/cluster name as needed or pass as parameters)
  environment {
    AWS_REGION        = 'ap-southeast-1'
    CLUSTER_NAME      = 'sd4719-eks-cluster'    // override via Jenkins job param or pipeline var
    K8S_APP_DIR       = 'kubernetes'
    K8S_MANIFEST_DIR  = "${K8S_APP_DIR}/aws-manifest-files"           // where your manifests live in the repo
    K8S_NAMESPACE     = 'default'
    NODE_MAX_HEAP     = 256                   // max heap for node builds
    TEMP_MANIFEST_DIR = "temp-aws-manifest-files"
    ARGOCD_SERVER     = "localhost:8080"          // ArgoCD server address
  }

  options {
    skipDefaultCheckout false
    timestamps()
    buildDiscarder(logRotator(numToKeepStr: '30')) // keep last 30 builds
  }

  parameters {
    string(name: 'IMAGE_TAG', defaultValue: '', description: 'Optional image tag. If empty, GIT_COMMIT or BUILD_NUMBER will be used.')
  }

  stages {
    stage('Checkout') {
      steps {
        checkout scm
        script {
          // resolve a stable image tag: prefer param => git short SHA => build number
          env.GIT_COMMIT_SHORT = sh(script: "git rev-parse --short HEAD", returnStdout: true).trim()
          if (params.IMAGE_TAG?.trim()) {
            env.IMAGE_TAG = params.IMAGE_TAG
          } else if (env.GIT_COMMIT_SHORT) {
            env.IMAGE_TAG = env.GIT_COMMIT_SHORT
          } else {
            env.IMAGE_TAG = env.BUILD_NUMBER
          }
          echo "Using IMAGE_TAG=${env.IMAGE_TAG}"
        }
      }
    }

    stage('Prepare AWS / ECR Info') {
      steps {
        // Bind AWS creds only for the steps that need them
        withCredentials([[
          $class: 'AmazonWebServicesCredentialsBinding',
          credentialsId: 'aws-creds'
        ]]) {
          sh """
            export AWS_REGION=${AWS_REGION}
            export AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
            export AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}

            ACCOUNT_ID=\$(aws sts get-caller-identity --query Account --output text --region ${AWS_REGION})
            echo "AWS account: \${ACCOUNT_ID}"
            echo "##teamcity[setParameter name='env.ACCOUNT_ID' value='\${ACCOUNT_ID}']" || true
            echo "ACCOUNT_ID=\${ACCOUNT_ID}" > account.env
          """
        }
        // load ACCOUNT_ID into env for subsequent stages
        script {
          def acct = readFile('account.env').trim().split('=')[-1]
          env.ACCOUNT_ID = acct
          env.ECR_REG = "${env.ACCOUNT_ID}.dkr.ecr.${env.AWS_REGION}.amazonaws.com"
          env.FRONT_IMAGE = "${env.ECR_REG}/frontend:${env.IMAGE_TAG}"
          env.BACK_IMAGE  = "${env.ECR_REG}/backend:${env.IMAGE_TAG}"
          echo "FRONT_IMAGE=${env.FRONT_IMAGE}"
          echo "BACK_IMAGE=${env.BACK_IMAGE}"
        }
      }
    }

    stage('ECR Login') {
      steps {
        withCredentials([[
          $class: 'AmazonWebServicesCredentialsBinding',
          credentialsId: 'aws-creds'
        ]]) {
          sh '''
            set -euo pipefail
            export AWS_REGION=${AWS_REGION}
            export AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
            export AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
            aws ecr get-login-password --region ${AWS_REGION} | docker login --username AWS --password-stdin ${ECR_REG}
            echo "Logged into ECR ${ECR_REG}"
          '''
        }
      }
    }

    // stage('Build & Push Images') {
    //   parallel {
        stage('Build & Push Frontend') {
          steps {
            dir("${K8S_APP_DIR}/src/frontend") {
              withCredentials([[
                $class: 'AmazonWebServicesCredentialsBinding',
                credentialsId: 'aws-creds'
              ]]) {
                sh '''
                  set -euo pipefail
                  export NODE_OPTIONS="--max-old-space-size=$NODE_MAX_HEAP"
                  export DOCKER_BUILDKIT=0

                  echo "Checking if FRONTEND image already exists in ECR..."

                  # Check number of images in repo
                  // IMAGE_COUNT=$(aws ecr describe-images \
                  //   --repository-name frontend \
                  //   --region $AWS_REGION \
                  //   --query 'length(imageDetails)' \
                  //   --output text 2>/dev/null)

                  # Check for specific tag 
                  IMAGE_EXISTS=$(aws ecr describe-images \
                    --repository-name frontend \
                    --region "$AWS_REGION" \
                    --query 'length(imageDetails?contains(imageTags, `'"${IMAGE_TAG}"'`))' \
                    --output text 2>/dev/null)
                    
                  # Checking image count is skipped to always build and push new images
                  // if [[ "$IMAGE_COUNT" -ge 1 ]]; then

                  # Check if image with the same tag exists
                  if [[ "$IMAGE_EXISTS" -ge 1 ]]; then
                    echo "Frontend image exists => SKIP build & push"
                  else
                    echo "No images found in frontend repository => BUILD + PUSH"

                    echo "Cleaning old Docker images..."
                    docker image prune -f
                    docker images | grep "<none>" || true | awk '{print $3}' | xargs -r docker rmi -f

                    docker build -t $FRONT_IMAGE .
                    docker push $FRONT_IMAGE
                  fi
                '''
              }
            }
          }
        }
        stage('Build & Push Backend') {
          steps {
            dir("${K8S_APP_DIR}/src/backend") {
              withCredentials([[
                $class: 'AmazonWebServicesCredentialsBinding',
                credentialsId: 'aws-creds'
              ]]) {
                sh '''
                  set -euo pipefail
                  export NODE_OPTIONS="--max-old-space-size=$NODE_MAX_HEAP"
                  export DOCKER_BUILDKIT=0

                  echo "Checking if BACKEND image already exists in ECR..."

                  # Check number of images in repo
                  // IMAGE_COUNT=$(aws ecr describe-images \
                  //   --repository-name backend \
                  //   --region $AWS_REGION \
                  //   --query 'length(imageDetails)' \
                  //   --output text 2>/dev/null)
                  
                  # Check for specific tag 
                  IMAGE_EXISTS=$(aws ecr describe-images \
                    --repository-name backend \
                    --region "$AWS_REGION" \
                    --query 'length(imageDetails?contains(imageTags, `'"${IMAGE_TAG}"'`))' \
                    --output text 2>/dev/null)
                    
                  # Checking image count is skipped to always build and push new images
                  // if [[ "$IMAGE_COUNT" -ge 1 ]]; then

                  # Check if image with the same tag exists
                  if [[ "$IMAGE_EXISTS" -ge 1 ]]; then
                    echo "Backend image exists => SKIP build & push"
                  else
                    echo "No images found in backend repository => BUILD + PUSH"

                    echo "Cleaning old Docker images..."
                    docker image prune -f
                    docker images | grep "<none>" || true | awk '{print $3}' | xargs -r docker rmi -f

                    docker build -t $BACK_IMAGE .
                    docker push $BACK_IMAGE
                  fi
                '''
              }
            }
          }
        }
    //    }
    // }

    // Note: The following stage is commented out because we are using ArgoCD to manage deployments.
    // If you prefer GitOps with ArgoCD, you can skip this stage.
    // stage('Prepare K8s Manifests') {
    //   steps {
    //     // Prefer kubectl rollout or ``kubectl set image`` instead of editing files then apply
    //     // We'll create a temporary copy of manifests and replace images there, then archive
    //     sh """
    //       set -euo pipefail
    //       mkdir -p ${TEMP_MANIFEST_DIR}

    //       echo "Copying manifests from: ${K8S_MANIFEST_DIR}"
    //       cp ${K8S_MANIFEST_DIR}/*.yaml ${TEMP_MANIFEST_DIR}/ || true

    //       echo "Patching manifest imagesâ€¦"

    //       # Update frontend image
    //       sed -i "s|image:.*frontend.*|image: ${env.FRONT_IMAGE}|g" ${TEMP_MANIFEST_DIR}/frontend.yaml || true

    //       # Update backend image
    //       sed -i "s|image:.*backend.*|image: ${env.BACK_IMAGE}|g"  ${TEMP_MANIFEST_DIR}/backend.yaml || true

    //       # keep the originals for traceability and archive the updated ones
    //       ls -la ${TEMP_MANIFEST_DIR}
    //     """
    //     archiveArtifacts artifacts: "${TEMP_MANIFEST_DIR}/*.yaml", fingerprint: true
    //   }
    // }

    // Note: The following stage is commented out because we are using ArgoCD to manage deployments.
    // If you prefer GitOps with ArgoCD, you can skip this stage.
    // stage('Deploy to EKS') {
    //   steps {
    //     withCredentials([[
    //       $class: 'AmazonWebServicesCredentialsBinding',
    //       credentialsId: 'aws-creds'
    //     ]]) {
    //       sh '''
    //         set -euo pipefail
    //         export AWS_REGION=$AWS_REGION
    //         export AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID
    //         export AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY

    //         # update kubeconfig for CLUSTER_NAME (must exist and IAM creds must be allowed)
    //         aws eks update-kubeconfig --region $AWS_REGION --name $CLUSTER_NAME

    //         # Option A: use kubectl apply on manifests (works for first deploy)
    //         kubectl apply --validate=false -f $TEMP_MANIFEST_DIR/mongodb.yaml || true
    //         kubectl apply --validate=false -f $TEMP_MANIFEST_DIR/backend.yaml
    //         kubectl apply --validate=false -f $TEMP_MANIFEST_DIR/frontend.yaml

    //         # Option B (preferred for live update): update deployments image
    //         # kubectl -n default set image deployment/backend backend=$BACK_IMAGE --record || true
    //         # kubectl -n default set image deployment/frontend frontend=$FRONT_IMAGE --record || true

    //         # wait for rollout to complete
    //         kubectl rollout status deployment/backend --timeout=120s || true
    //         kubectl rollout status deployment/frontend --timeout=120s || true
    //       '''
    //     }
    //   }
    // }

    // New stage to deploy via ArgoCD
    // Commnent out this stage if you prefer direct kubectl deployment
    // stage('Deploy to EKS via ArgoCD') {
    //   steps {
    //      withCredentials([string(credentialsId: 'argocd-token', variable: 'ARGOCD_TOKEN')]) {
    //       sh '''
    //         # Login to Argo CD
    //         # argocd login $ARGOCD_SERVER --username admin --password $ARGOCD_TOKEN --core
    //         argocd login $ARGOCD_SERVER \
    //         --username admin \
    //         --password $ARGOCD_TOKEN \
    //         --insecure \
    //         --grpc-web

    //         # Sync ArgoCD Application
    //         argocd app sync my-app-argocd --grpc-web --timeout 300 --wait

    //         argocd app get my-app-argocd --core
    //       '''
    //     }
    //   }
    // }

    // If using ArgoCD to manage deployments, this stage provides a summary
    stage('Post-Build Summary') {
      steps {
        echo """
          GitOps Build Complete

          Frontend Image: ${env.FRONT_IMAGE}
          Backend Image : ${env.BACK_IMAGE}

          ArgoCD Image Updater will detect this new
          image tag in ECR and update the GitOps repo.
          ArgoCD Auto-Sync will deploy the change.
          """
      }
    }
    
    stage('Verify') {
      steps {
        sh '''
          echo "Pods (default):"
          kubectl get pods -n $K8S_NAMESPACE -o wide || true
          echo "Services:"
          kubectl get svc -n default || true
        '''
      }
    }
  } // stages

  post {
    success {
      echo "Deployment succeeded: frontend => ${env.FRONT_IMAGE}, backend => ${env.BACK_IMAGE}"
    }
    failure {
      sh 'kubectl get pods --all-namespaces || true'
      echo "Deployment failed. Check console output and logs."
    }
    cleanup {
      // Optional cleanup: remove local images to save disk
      sh 'docker image prune -af || true'
    }
  }
}
